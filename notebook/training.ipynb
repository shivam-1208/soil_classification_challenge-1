{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ðŸš€ Optimized Soil Classification Training Pipeline","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import convnext_tiny\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:21:26.569701Z","iopub.execute_input":"2025-05-23T04:21:26.570015Z","iopub.status.idle":"2025-05-23T04:21:36.154937Z","shell.execute_reply.started":"2025-05-23T04:21:26.569988Z","shell.execute_reply":"2025-05-23T04:21:36.154250Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# âœ… Device config","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:21:42.385380Z","iopub.execute_input":"2025-05-23T04:21:42.385815Z","iopub.status.idle":"2025-05-23T04:21:42.392333Z","shell.execute_reply.started":"2025-05-23T04:21:42.385793Z","shell.execute_reply":"2025-05-23T04:21:42.391657Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"\n# ðŸ”§ Kaggle credentials","metadata":{}},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = 'shivam790'\nos.environ['KAGGLE_KEY'] = '382c7bf4cacced2e14d0360737efc6c9'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:21:45.289917Z","iopub.execute_input":"2025-05-23T04:21:45.290201Z","iopub.status.idle":"2025-05-23T04:21:45.294234Z","shell.execute_reply.started":"2025-05-23T04:21:45.290183Z","shell.execute_reply":"2025-05-23T04:21:45.293466Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# ðŸ“¥ Download and Extract Dataset","metadata":{}},{"cell_type":"code","source":"!kaggle competitions download -c soil-classification\nwith zipfile.ZipFile(\"soil-classification.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"soil_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:21:48.419177Z","iopub.execute_input":"2025-05-23T04:21:48.420010Z","iopub.status.idle":"2025-05-23T04:21:55.750540Z","shell.execute_reply.started":"2025-05-23T04:21:48.419982Z","shell.execute_reply":"2025-05-23T04:21:55.749502Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# ðŸ“„ Load and encode labels","metadata":{}},{"cell_type":"code","source":"base_path = \"soil_data/soil_classification-2025\"\ntrain_csv = os.path.join(base_path, \"train_labels.csv\")\ntest_csv = os.path.join(base_path, \"test_ids.csv\")\ntrain_dir = os.path.join(base_path, \"train\")\ntest_dir = os.path.join(base_path, \"test\")\n\n# ðŸ“„ Load and encode labels\ntrain_df = pd.read_csv(train_csv)\ntrain_df[\"image_path\"] = train_df[\"image_id\"].apply(lambda x: os.path.join(train_dir, x))\nlabel_encoder = LabelEncoder()\ntrain_df[\"label\"] = label_encoder.fit_transform(train_df[\"soil_type\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:01.790561Z","iopub.execute_input":"2025-05-23T04:22:01.790873Z","iopub.status.idle":"2025-05-23T04:22:01.819062Z","shell.execute_reply.started":"2025-05-23T04:22:01.790845Z","shell.execute_reply":"2025-05-23T04:22:01.818142Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# ðŸ“‚ Dataset classes","metadata":{}},{"cell_type":"code","source":"class SoilDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['image_path']\n        label = self.df.iloc[idx]['label']\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\nclass TestSoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['image_id']\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:05.529804Z","iopub.execute_input":"2025-05-23T04:22:05.530112Z","iopub.status.idle":"2025-05-23T04:22:05.538875Z","shell.execute_reply.started":"2025-05-23T04:22:05.530088Z","shell.execute_reply":"2025-05-23T04:22:05.538067Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# ðŸŽ¨ Transforms","metadata":{}},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.AutoAugment(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:10.112972Z","iopub.execute_input":"2025-05-23T04:22:10.113262Z","iopub.status.idle":"2025-05-23T04:22:10.119192Z","shell.execute_reply.started":"2025-05-23T04:22:10.113240Z","shell.execute_reply":"2025-05-23T04:22:10.118330Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# ðŸ”€ Split data","metadata":{}},{"cell_type":"code","source":"train_split, val_split = train_test_split(train_df, test_size=0.15, stratify=train_df['label'])\ntrain_dataset = SoilDataset(train_split, transform_train)\nval_dataset = SoilDataset(val_split, transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:13.336883Z","iopub.execute_input":"2025-05-23T04:22:13.337152Z","iopub.status.idle":"2025-05-23T04:22:13.351092Z","shell.execute_reply.started":"2025-05-23T04:22:13.337134Z","shell.execute_reply":"2025-05-23T04:22:13.350147Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"\n# ðŸ§  Load ConvNeXt Tiny","metadata":{}},{"cell_type":"code","source":"model = convnext_tiny(weights='IMAGENET1K_V1')\nmodel.classifier[2] = nn.Linear(model.classifier[2].in_features, len(label_encoder.classes_))\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:16.762968Z","iopub.execute_input":"2025-05-23T04:22:16.763234Z","iopub.status.idle":"2025-05-23T04:22:18.220508Z","shell.execute_reply.started":"2025-05-23T04:22:16.763217Z","shell.execute_reply":"2025-05-23T04:22:18.219758Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109M/109M [00:00<00:00, 174MB/s] \n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=4, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# âš™ï¸ Training loop with AMP + F1 tracking\ndef train_model(model, train_loader, val_loader, epochs=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2)\n    scaler = GradScaler()\n\n    best_f1 = 0.0\n    for epoch in range(epochs):\n        model.train()\n        train_preds, train_labels = [], []\n        running_loss = 0.0\n\n        for images, labels in tqdm(train_loader):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            running_loss += loss.item()\n            train_preds.extend(outputs.argmax(1).cpu().numpy())\n            train_labels.extend(labels.cpu().numpy())\n\n        train_f1 = f1_score(train_labels, train_preds, average='macro')\n        print(f\"Epoch {epoch+1}: Train Loss={running_loss/len(train_loader):.4f}, F1={train_f1:.4f}\")\n\n        # Validation\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                val_preds.extend(outputs.argmax(1).cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n        val_f1 = f1_score(val_labels, val_preds, average='macro')\n        print(f\"Validation F1 Score: {val_f1:.4f}\")\n        scheduler.step(val_f1)\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n    return model      \n \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:32.626910Z","iopub.execute_input":"2025-05-23T04:22:32.627214Z","iopub.status.idle":"2025-05-23T04:22:32.636779Z","shell.execute_reply.started":"2025-05-23T04:22:32.627194Z","shell.execute_reply":"2025-05-23T04:22:32.635723Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ðŸš‚ Train\nmodel = train_model(model, train_loader, val_loader, epochs=5)\n\n# ðŸ§ª Load test data\ntest_df = pd.read_csv(test_csv)\ntest_dataset = TestSoilDataset(test_df, test_dir, transform_test)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# ðŸ”® Predict\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\npreds = []\nwith torch.no_grad():\n    for images in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        probs = F.softmax(outputs, dim=1)\n        preds.append(probs.cpu())\n\nall_probs = torch.cat(preds, dim=0)\nfinal_preds = torch.argmax(all_probs, dim=1)\nfinal_labels = label_encoder.inverse_transform(final_preds.numpy())\n\n# ðŸ“¤ Submission\nsubmission = pd.DataFrame({\n    \"image_id\": test_df[\"image_id\"],\n    \"soil_type\": final_labels\n})\nsubmission.to_csv(\"optimized_submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:22:40.019319Z","iopub.execute_input":"2025-05-23T04:22:40.019624Z","iopub.status.idle":"2025-05-23T04:53:52.571175Z","shell.execute_reply.started":"2025-05-23T04:22:40.019604Z","shell.execute_reply":"2025-05-23T04:53:52.570134Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/4074699225.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipykernel_35/4074699225.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [05:54<00:00, 10.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6302, F1=0.7592\nValidation F1 Score: 0.9478\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipykernel_35/4074699225.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [05:43<00:00, 10.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2333, F1=0.9218\nValidation F1 Score: 0.9482\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipykernel_35/4074699225.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [05:51<00:00, 10.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.1865, F1=0.9307\nValidation F1 Score: 0.9537\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipykernel_35/4074699225.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [05:46<00:00, 10.49s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1286, F1=0.9491\nValidation F1 Score: 0.9870\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipykernel_35/4074699225.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [05:44<00:00, 10.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1006, F1=0.9564\nValidation F1 Score: 0.9866\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}